{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIRCV_Functions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarshaGomez/DNN-Sketches-image-analysis/blob/main/Code/MIRCV_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5Q2es59P8pT"
      },
      "source": [
        "import numpy as np\r\n",
        "import time\r\n",
        "import itertools\r\n",
        "import os, shutil\r\n",
        "import tensorflow as tf\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "from random import random\r\n",
        "from ipywidgets import Image\r\n",
        "from numpy.linalg import norm\r\n",
        "from IPython.display import display\r\n",
        "from keras.models import Model\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "\r\n",
        "# tensorflow version 2.4.0\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers, models\r\n",
        "from tensorflow.keras.optimizers import SGD, Adam\r\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.preprocessing import image\r\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input, InceptionV3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYlyY3OYLxCx"
      },
      "source": [
        "## Copy Tree\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6rnmMDiKaf_"
      },
      "source": [
        "def copytree(src, dst, symlinks=False, ignore=None):\r\n",
        "    for item in os.listdir(src):\r\n",
        "        s = os.path.join(src, item)\r\n",
        "        d = os.path.join(dst, item)\r\n",
        "        if os.path.isdir(s):\r\n",
        "            shutil.copytree(s, d, symlinks, ignore)\r\n",
        "        else:\r\n",
        "            shutil.copy2(s, d)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX8MEBrCKspj"
      },
      "source": [
        "##    LSH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Vg85_q2KsW5"
      },
      "source": [
        "class LSH:\r\n",
        "  def __init__(self, feature_dim, g = 10, h = 20, w = 4, bitwise_hash = False):\r\n",
        "    \"\"\"\r\n",
        "    We have to find a way to load the stored index if exists or initialize the \r\n",
        "    initial structure\r\n",
        "    \"\"\"\r\n",
        "    self._index = {}\r\n",
        "    if bitwise_hash:\r\n",
        "      # self.x = np.random.randn(g, h, feature_dim) # non sembra richieda una distribuzione normale\r\n",
        "      self.x = np.random.normal(size=(g, h, feature_dim))\r\n",
        "    else:\r\n",
        "      self.x = np.random.normal(size=(g, h, feature_dim))\r\n",
        "    self.w = np.ones((g, h, 1)) * w\r\n",
        "    self.b = np.random.rand(g, h, 1) * w\r\n",
        "    self.bitwise_hash = bitwise_hash # another way to create h (h will be only 0 o 1)\r\n",
        "\r\n",
        "  def _hash(self, features):\r\n",
        "    \"\"\"\r\n",
        "    crea l'hash di più cose contemporaeamente si aspetta un array composto dalle features una sotta l'altra (linguaggio super matematico)\r\n",
        "    \"\"\"\r\n",
        "    # g = np.trunc((np.dot(p, self.x) + self.b) / self.w)\r\n",
        "    #g = np.trunc((np.dot(self.x, p) + self.b) / self.w) questo funziona con 1\r\n",
        "    # g = np.transpose(np.trunc(((np.dot(self.x, p.T) + self.b) / self.w)), (0,2,1)) miglior modo di vederlo\r\n",
        "    if self.bitwise_hash:\r\n",
        "      return (np.transpose(np.dot(self.x, features.T), (0,2,1)) > 0).astype(int).astype(str)\r\n",
        "    return np.transpose(np.trunc(((np.dot(self.x, features.T) + self.b) / self.w)), (0,2,1)).astype(int).astype(str)\r\n",
        "\r\n",
        "  def insert(self, features, ids, labels):\r\n",
        "    \"\"\"\r\n",
        "    Insert new data, ci aspettiamo un array d\r\n",
        "    \"\"\"\r\n",
        "    g = self._hash(features)\r\n",
        "    assert features.shape[0] == len(ids), \"mismatch between ids length and features\"\r\n",
        "    assert len(labels) == len(ids), \"mismatch between ids length and labels\"\r\n",
        "    \r\n",
        "    number_elements = len(ids)\r\n",
        "    i = 0\r\n",
        "    # print(\"hash calculated\")\r\n",
        "    # print(g.shape)\r\n",
        "    g_index = -1\r\n",
        "    for g_function in g:\r\n",
        "      start_inner_for = time.time()\r\n",
        "      g_index += 1\r\n",
        "      for row in g_function:\r\n",
        "        if i % 10000 == 0:\r\n",
        "          start = time.time()\r\n",
        "        \r\n",
        "        bucket_id = str(g_index) + '_' + ','.join(row)\r\n",
        "        \r\n",
        "        if i % 10000 == 0:\r\n",
        "            end = time.time()\r\n",
        "            # print(f'join {end - start}')\r\n",
        "        if not bucket_id in self._index:\r\n",
        "          # self._index[bucket_id] = { 'features': np.array([features[i % number_elements]]), 'ids': np.array([ids[i % number_elements]]), 'labels': np.array([labels[i % number_elements]])}\r\n",
        "          self._index[bucket_id] = { 'features': [features[i % number_elements]], 'ids': [ids[i % number_elements]], 'labels': [labels[i % number_elements]]}\r\n",
        "          if i % 10000 == 0:\r\n",
        "            end = time.time()\r\n",
        "            # print(f'not in bucket {end - start}')\r\n",
        "        else:\r\n",
        "          if ids[i % number_elements] in self._index[bucket_id]['ids']:\r\n",
        "            # print(\"duplicate\")\r\n",
        "            continue\r\n",
        "          if i % 10000 == 0:\r\n",
        "            end = time.time()\r\n",
        "            # print(f'checking duplicates {end - start}')\r\n",
        "          # print(\"collision inserted\")\r\n",
        "          # self._index[bucket_id]['features'] = np.vstack((self._index[bucket_id]['features'], features[i % number_elements]))\r\n",
        "          self._index[bucket_id]['features'].append(features[i % number_elements])\r\n",
        "          # self._index[bucket_id]['ids'] = np.vstack((self._index[bucket_id]['ids'], ids[i % number_elements]))\r\n",
        "          self._index[bucket_id]['ids'].append(ids[i % number_elements])\r\n",
        "          # self._index[bucket_id]['labels'] = np.vstack((self._index[bucket_id]['labels'], labels[i % number_elements]))\r\n",
        "          self._index[bucket_id]['labels'].append(labels[i % number_elements])\r\n",
        "          if i % 10000 == 0:\r\n",
        "            end = time.time()\r\n",
        "            # print(f'stacking {end - start}')\r\n",
        "        i += 1\r\n",
        "        assert i > 0, 'out of bound'\r\n",
        "      end_inner_for = time.time()\r\n",
        "      # print(f'inner for time: {end_inner_for - start_inner_for}')\r\n",
        "    \r\n",
        "    for bucket_id in self._index:\r\n",
        "      self._index[bucket_id]['features'] = np.array(self._index[bucket_id]['features'])\r\n",
        "      # print(self._index[bucket_id]['features'].shape)\r\n",
        "      self._index[bucket_id]['ids'] = np.array(self._index[bucket_id]['ids'], )\r\n",
        "      self._index[bucket_id]['labels'] = np.array(self._index[bucket_id]['labels'])\r\n",
        "      \r\n",
        "\r\n",
        "  def query(self, features, top_k, mode = 'euclidean', return_cost = False):\r\n",
        "    \"\"\"\r\n",
        "    Query the data\r\n",
        "    \"\"\"\r\n",
        "    g = self._hash(np.array([features]))\r\n",
        "    i = 0\r\n",
        "    k = None\r\n",
        "    top_k += 1 # per far ritornare k e non k - 1\r\n",
        "    cost = 0\r\n",
        "    g_index = -1\r\n",
        "    assert mode in ['similarity', 'euclidean'], \"mode must be similarity or euclidean\"\r\n",
        "    for g_function in g:\r\n",
        "      g_index += 1\r\n",
        "      for row in g_function:\r\n",
        "        bucket_id = str(g_index) + '_' + ','.join(row)\r\n",
        "        # print(bucket_id)\r\n",
        "        if bucket_id in self._index:\r\n",
        "          # posso avere duplicati perchè se i punti vengono inseriti in più bucket, posso avere duplicati\r\n",
        "          # quindi devo eliminarli\r\n",
        "          # l'ho messo qua fuori che il controllo duplicati è uguale per tutte e due le distanze\r\n",
        "          # print(f'bucket {bucket_id}')\r\n",
        "          if k is not None:\r\n",
        "            # print(\"duplicate\")\r\n",
        "            duplicate_index = np.isin(self._index[bucket_id]['ids'], k['ids'])\r\n",
        "            if duplicate_index.all():\r\n",
        "              continue; # se sono tutti duplicati non ha senso contare nulla\r\n",
        "            bucket = {}\r\n",
        "            bucket['ids'] = self._index[bucket_id]['ids'][~duplicate_index] # prendo quelli che non sono duplicati\r\n",
        "            # print(duplicate_index)\r\n",
        "            # print(self._index[bucket_id]['features'])\r\n",
        "            bucket['features'] = self._index[bucket_id]['features'][~duplicate_index.flatten()] # each duplicate index must delete a row of features\r\n",
        "            bucket['labels'] = self._index[bucket_id]['labels'][~duplicate_index]\r\n",
        "          else:\r\n",
        "            bucket = self._index[bucket_id]\r\n",
        "        \r\n",
        "          if mode == 'euclidean':\r\n",
        "            # print(bucket['features'].shape)\r\n",
        "            dist = norm(bucket['features'] - np.array(features), axis=1)\r\n",
        "            # print(f'dist shape {dist.shape} and dist size {dist.size}')\r\n",
        "            cost += dist.size\r\n",
        "            if k is None:\r\n",
        "              idx_partitioned = np.argpartition(dist, top_k - 1 if dist.shape[0] - 1 > top_k - 1 else dist.shape[0] - 1)\r\n",
        "              if dist.shape[0] - 1 > top_k - 1:  \r\n",
        "                  idx_partitioned = idx_partitioned[:top_k - 1]\r\n",
        "              k = {}\r\n",
        "              # qua è più comodo avere array 1- dimensionali\r\n",
        "              k['ids'] = bucket['ids'][idx_partitioned].flatten()\r\n",
        "              k['labels'] = bucket['labels'][idx_partitioned].flatten()\r\n",
        "              k['distances'] = dist[idx_partitioned]\r\n",
        "              continue\r\n",
        "            # https://stackoverflow.com/questions/10337533/a-fast-way-to-find-the-largest-n-elements-in-an-numpy-array\r\n",
        "            # argpartition sembra essere incredibilmente veloce\r\n",
        "            # ma non ordina completamente, ordina solo rispetto un punto, nel senso\r\n",
        "            # io gli sto dicendo butta quelli più piccoli di k da una parte e quelli più grandi all'altra, ma non sto ordinando\r\n",
        "            if k['distances'].shape[0] < top_k:\r\n",
        "                # print((k['distances'].shape, dist.shape))\r\n",
        "                distances = np.concatenate((k['distances'], dist))\r\n",
        "                # print((k['ids'].shape, bucket['ids'].shape))\r\n",
        "                # print(k['ids'])\r\n",
        "                # print(bucket['ids'])\r\n",
        "                ids = np.concatenate((k['ids'], bucket['ids']))\r\n",
        "                # print((k['labels'].shape, bucket['labels'].shape))\r\n",
        "                labels = np.concatenate((k['labels'], bucket['labels']))\r\n",
        "                idx_sorted = np.argpartition(distances, top_k - 1 if distances.shape[0] - 1 > top_k else distances.shape[0] - 1)\r\n",
        "                if distances.shape[0] - 1 > top_k - 1:  \r\n",
        "                  idx_sorted = idx_sorted[:top_k - 1]\r\n",
        "                k['ids'] = ids[idx_sorted]\r\n",
        "                k['labels'] = labels[idx_sorted]\r\n",
        "                k['distances'] = distances[idx_sorted]\r\n",
        "                # print(f'k = {k}')\r\n",
        "                continue\r\n",
        "\r\n",
        "            idx = dist < np.max(k['distances'])\r\n",
        "            # print(f\"idx = {idx}\")\r\n",
        "            if np.any(idx):\r\n",
        "              distances = np.concatenate((k['distances'], dist[idx]))\r\n",
        "              ids = np.concatenate((k['ids'], bucket['ids'][idx]))\r\n",
        "              labels = np.concatenate((k['labels'], bucket['labels'][idx]))\r\n",
        "              idx_sorted = np.argpartition(distances, top_k - 1 if distances.shape[0] - 1 > top_k else distances.shape[0] - 1)\r\n",
        "              if distances.shape[0] - 1 > top_k - 1:  \r\n",
        "                  idx_sorted = idx_sorted[:top_k - 1]\r\n",
        "              k['ids'] = ids[idx_sorted]\r\n",
        "              k['labels'] = labels[idx_sorted]\r\n",
        "              k['distances'] = distances[idx_sorted]\r\n",
        "\r\n",
        "          else:\r\n",
        "            # print(bucket['features'].shape)\r\n",
        "            sim = np.sum(bucket['features'] * np.array(features), axis=1) / (norm(bucket['features'], axis=1) * norm(np.array([features]), axis=1))\r\n",
        "            # print(f'sim shape {sim.shape} and sim size {sim.size}')\r\n",
        "            cost += sim.size\r\n",
        "            if k is None:\r\n",
        "              idx_partitioned = np.argpartition(sim, -(top_k - 1) if sim.shape[0] - 1 > top_k - 1 else sim.shape[0] - 1)\r\n",
        "              if sim.shape[0] - 1 > top_k - 1:  \r\n",
        "                idx_partitioned = idx_partitioned[-(top_k - 1):]\r\n",
        "              k = {}\r\n",
        "              # qua è più comodo avere array 1- dimensionali\r\n",
        "              k['ids'] = bucket['ids'][idx_partitioned].flatten()\r\n",
        "              k['labels'] = bucket['labels'][idx_partitioned].flatten()\r\n",
        "              k['similarities'] = sim[idx_partitioned]\r\n",
        "              continue\r\n",
        "            # https://stackoverflow.com/questions/10337533/a-fast-way-to-find-the-largest-n-elements-in-an-numpy-array\r\n",
        "            # argpartition sembra essere incredibilmente veloce\r\n",
        "            # ma non ordina completamente, ordina solo rispetto un punto, nel senso\r\n",
        "            # io gli sto dicendo butta quelli più piccoli di k da una parte e quelli più grandi all'altra, ma non sto ordinando\r\n",
        "            if k['similarities'].shape[0] < top_k:\r\n",
        "                # print((k['similarities'].shape, sim.shape))\r\n",
        "                similarities = np.concatenate((k['similarities'], sim))\r\n",
        "                # print((k['ids'].shape, bucket['ids'].shape))\r\n",
        "                # print(k['ids'])\r\n",
        "                # print(bucket['ids'])\r\n",
        "                ids = np.concatenate((k['ids'], bucket['ids']))\r\n",
        "                # print((k['labels'].shape, bucket['labels'].shape))\r\n",
        "                labels = np.concatenate((k['labels'], bucket['labels']))\r\n",
        "                idx_sorted = np.argpartition(similarities, -(top_k - 1) if similarities.shape[0] - 1 > top_k - 1 else similarities.shape[0] - 1)\r\n",
        "                if similarities.shape[0] - 1 > top_k - 1:  \r\n",
        "                  idx_sorted = idx_sorted[-(top_k - 1):]\r\n",
        "                k['ids'] = ids[idx_sorted]\r\n",
        "                k['labels'] = labels[idx_sorted]\r\n",
        "                k['similarities'] = similarities[idx_sorted]\r\n",
        "                # print(f'k = {k}')\r\n",
        "                continue\r\n",
        "\r\n",
        "            idx = sim > np.min(k['similarities'])\r\n",
        "            # print(f\"idx = {idx}\")\r\n",
        "            if np.any(idx):\r\n",
        "              similarities = np.concatenate((k['similarities'], sim[idx]))\r\n",
        "              ids = np.concatenate((k['ids'], bucket['ids'][idx]))\r\n",
        "              labels = np.concatenate((k['labels'], bucket['labels'][idx]))\r\n",
        "              idx_sorted = np.argpartition(similarities, -(top_k - 1) if similarities.shape[0] - 1 > top_k - 1 else similarities.shape[0] - 1)\r\n",
        "              if similarities.shape[0] - 1 > top_k - 1:  \r\n",
        "                idx_sorted = idx_sorted[-(top_k - 1):]\r\n",
        "              k['ids'] = ids[idx_sorted]\r\n",
        "              k['labels'] = labels[idx_sorted]\r\n",
        "              k['similarities'] = similarities[idx_sorted]\r\n",
        "        i += 1\r\n",
        "    # ora ordino totalmente i risultati\r\n",
        "    if k is None:\r\n",
        "      return {} #zero result\r\n",
        "    if mode == 'euclidean':\r\n",
        "      idx_sorted = np.argsort(k['distances'])\r\n",
        "      idx_sorted = idx_sorted[:top_k - 1 if k['distances'].shape[0] - 1 > top_k else k['distances'].shape[0]]\r\n",
        "      k['distances'] = k['distances'][idx_sorted]\r\n",
        "      k['ids'] = k['ids'][idx_sorted]\r\n",
        "      k['labels'] = k['labels'][idx_sorted]\r\n",
        "      if return_cost:\r\n",
        "        return (k, cost)\r\n",
        "      return k\r\n",
        "    idx_sorted = np.argsort(k['similarities'])[::-1]\r\n",
        "    idx_sorted = idx_sorted[:top_k - 1 if k['similarities'].shape[0] - 1 > top_k else k['similarities'].shape[0]]\r\n",
        "    k['similarities'] = k['similarities'][idx_sorted]\r\n",
        "    k['ids'] = k['ids'][idx_sorted]\r\n",
        "    k['labels'] = k['labels'][idx_sorted]\r\n",
        "    if return_cost:\r\n",
        "      return (k, cost)\r\n",
        "    return k\r\n",
        "\r\n",
        "\r\n",
        "def store(self):\r\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaRIOCyoK85s"
      },
      "source": [
        "## NO INDEX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtOTOJdqK8tM"
      },
      "source": [
        "class NO_INDEX:\r\n",
        "  def __init__(self):\r\n",
        "    self.features = None\r\n",
        "    self.ids = None\r\n",
        "    self.labels = None\r\n",
        "\r\n",
        "  def insert(self, features, ids, labels):\r\n",
        "    assert features.shape[0] == len(ids), \"mismatch between ids length and features\"\r\n",
        "    assert len(labels) == len(ids), \"mismatch between ids length and labels\"\r\n",
        "    self.features = features\r\n",
        "    self.ids = ids\r\n",
        "    self.labels = labels\r\n",
        "  \r\n",
        "  def query(self, features, top_k, mode = 'euclidean', return_cost = False):\r\n",
        "    assert mode in ['similarity', 'euclidean'], \"mode must be similarity or euclidean\"\r\n",
        "    top_k += 1\r\n",
        "    if mode == 'euclidean':\r\n",
        "      dist = norm(self.features - np.array(features), axis=1)\r\n",
        "      idx_partitioned = np.argpartition(dist, top_k - 1 if dist.shape[0] - 1 > top_k - 1 else dist.shape[0] - 1)\r\n",
        "      k = {}\r\n",
        "      # qua è più comodo avere array 1- dimensionali\r\n",
        "      k['ids'] = self.ids[idx_partitioned].flatten()[:top_k - 1 if dist.shape[0] - 1 > top_k - 1 else dist.shape[0]]\r\n",
        "      k['labels'] = self.labels[idx_partitioned].flatten()[:top_k - 1 if dist.shape[0] - 1 > top_k - 1 else dist.shape[0]]\r\n",
        "      k['distances'] = dist[idx_partitioned][:top_k - 1 if dist.shape[0] - 1 > top_k - 1 else dist.shape[0]]\r\n",
        "      \r\n",
        "      idx_sorted = np.argsort(k['distances'])\r\n",
        "      idx_sorted = idx_sorted\r\n",
        "      k['distances'] = k['distances'][idx_sorted]\r\n",
        "      k['ids'] = k['ids'][idx_sorted]\r\n",
        "      k['labels'] = k['labels'][idx_sorted]\r\n",
        "      if return_cost:\r\n",
        "        return (k, dist.size)\r\n",
        "      return k\r\n",
        "    sim = np.sum(self.features * np.array(features), axis=1) / (norm(self.features, axis=1) * norm(np.array([features]), axis=1))\r\n",
        "    idx_partitioned = np.argpartition(sim, -(top_k - 1) if sim.shape[0] - 1 > top_k - 1 else sim.shape[0] - 1)\r\n",
        "    k = {}\r\n",
        "    # qua è più comodo avere array 1- dimensionali\r\n",
        "    if sim.shape[0] - 1 > top_k - 1:  \r\n",
        "      idx_partitioned = idx_partitioned[-(top_k - 1):]\r\n",
        "\r\n",
        "    k['ids'] = self.ids[idx_partitioned].flatten()\r\n",
        "    k['labels'] = self.labels[idx_partitioned].flatten()\r\n",
        "    k['similarities'] = sim[idx_partitioned]\r\n",
        "    idx_sorted = np.argsort(k['similarities'])[::-1]\r\n",
        "    idx_sorted = idx_sorted\r\n",
        "    k['similarities'] = k['similarities'][idx_sorted]\r\n",
        "    k['ids'] = k['ids'][idx_sorted]\r\n",
        "    k['labels'] = k['labels'][idx_sorted]\r\n",
        "    if return_cost:\r\n",
        "      return (k, sim.size)\r\n",
        "    return k\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39lFY_bBLHHU"
      },
      "source": [
        "## Extract Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAtv3RhQLG89"
      },
      "source": [
        "#Extracting features using the pretrained convolutional base \r\n",
        "\r\n",
        "def extract_features(extractor, generator, sample_count, dim=2048):\r\n",
        "  features = np.zeros((sample_count, dim)) #extractor output shape \r\n",
        "  i = 0\r\n",
        "  for inputs_batch, labels_batch in generator:\r\n",
        "    start = time.time()\r\n",
        "    features_batch = extractor.predict(inputs_batch)\r\n",
        "    start = time.time()\r\n",
        "    if (i + 1) * BATCH_SIZE > sample_count:\r\n",
        "      features[i * BATCH_SIZE : sample_count , :] = features_batch\r\n",
        "      assert np.array_equal(np.argmax(labels_batch, axis = 1), generator.labels[i * BATCH_SIZE : sample_count]), 'LABELS NOT CORRESPONDING REINIZIALIZE GENERATOR'\r\n",
        "    else:\r\n",
        "      features[i * BATCH_SIZE : (i + 1) * BATCH_SIZE, : ] = features_batch\r\n",
        "      assert np.array_equal(np.argmax(labels_batch, axis = 1), generator.labels[i * BATCH_SIZE : (i + 1) * BATCH_SIZE]), 'LABELS NOT CORRESPONDING REINIZIALIZE GENERATOR'\r\n",
        "    i += 1\r\n",
        "    if i * BATCH_SIZE >= sample_count:\r\n",
        "      break\r\n",
        "  \r\n",
        "  return features\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By3ZJLSsLQba"
      },
      "source": [
        "## Compare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHnTQGixLPxf"
      },
      "source": [
        "def compare(a, b):\r\n",
        "  for comp in zip(a, b):\r\n",
        "    if comp[0] != comp[1]:\r\n",
        "      print(comp)\r\n",
        "      return False\r\n",
        "  return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lsw4cMaLbPe"
      },
      "source": [
        "## Average Precision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbFkKcxALa9s"
      },
      "source": [
        "def average_precision(requested_label, result_labels, n_ground_truth = 80):\r\n",
        "  \"\"\"\r\n",
        "  label ricercata, label ottenute, il numero di oggetti che ci sono quella label\r\n",
        "  \"\"\"\r\n",
        "  correct_array = (requested_label == result_labels).astype(int)\r\n",
        "  precision_array = [np.mean(correct_array[:k]) for k in range(1, correct_array.shape[0] + 1)]\r\n",
        "  # print(precision_array * correct_array) # mi rimangono solo quelli a 1\r\n",
        "  return np.sum(precision_array * correct_array) / n_ground_truth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx927OyELf3y"
      },
      "source": [
        "## mAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lauRtaxULfsT"
      },
      "source": [
        "def mAP(index, features, n_queries = 250, n_labels = 250, img_per_labels = 80, mode = 'euclidean'):\r\n",
        "  sum = 0\r\n",
        "  for i in range(n_queries):\r\n",
        "    label = i % n_labels\r\n",
        "    image_idx = ((i * img_per_labels) + int(random() * img_per_labels)) % (n_labels * img_per_labels)\r\n",
        "    # print('QUERY')\r\n",
        "    # print('index = ' + str(image_idx))\r\n",
        "    # print('label =' + str(label))\r\n",
        "    res = index.query(features[image_idx], img_per_labels, mode = mode)\r\n",
        "    # print('first label of resultset (must be equal to label) = ' + str(res['labels'][0]))\r\n",
        "    assert res['labels'][0] == label, 'deve essere della stessa label'\r\n",
        "    a = average_precision(label, res['labels'], img_per_labels)\r\n",
        "    sum += a\r\n",
        "  return sum / n_queries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txQGfvddLmW3"
      },
      "source": [
        "## Bucket Dispersion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xce_NsWLlf6"
      },
      "source": [
        "# try to calculate bucket dispersion\r\n",
        "# ritorna la percentuale media, la deviazione standard della classe più popolosa dei bucket,\r\n",
        "# ritorna anche il numero di bucket\r\n",
        "def bucket_dispersion(index):\r\n",
        "  percs = None\r\n",
        "  itemsCount = None\r\n",
        "  i = 0\r\n",
        "  for bucket in index._index:\r\n",
        "    i += 1\r\n",
        "    _, counts = np.unique(index._index[bucket]['labels'], return_counts=True)\r\n",
        "    perc = np.max(counts) / np.sum(counts)\r\n",
        "    itemCount = np.sum(counts)\r\n",
        "    if percs is None:\r\n",
        "      percs = np.array([perc])\r\n",
        "      itemsCount = np.array([itemCount])\r\n",
        "    else:\r\n",
        "      percs = np.concatenate((percs, np.array([perc])))\r\n",
        "      itemsCount = np.concatenate((itemsCount, np.array([itemCount])))\r\n",
        "  return {\r\n",
        "      'perc': {\r\n",
        "          'mean': np.mean(percs),\r\n",
        "          'deviation': np.std(percs)\r\n",
        "      },\r\n",
        "      'count': {\r\n",
        "          'average': np.mean(itemsCount),\r\n",
        "          'deviation': np.std(itemsCount)\r\n",
        "      },\r\n",
        "      'n_buckets': i,\r\n",
        "      'n_items_counting_duplicates': np.sum(itemsCount) # ci sono anche i duplicati\r\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}