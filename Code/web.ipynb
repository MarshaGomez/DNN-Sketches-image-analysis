{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarshaGomez/DNN-Sketches-image-analysis/blob/main/Code/web.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5Q8mE7JMSce"
      },
      "source": [
        "# Setting up the projection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOOOEXKKTemo",
        "outputId": "d1dc4652-71dc-4fca-8f7b-8fe10a7fb461"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItwcOvX1SCcV",
        "outputId": "64f6e738-e9ef-4567-fa09-145a01baa508"
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEfqD9P_R2Vf"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import itertools\n",
        "import os, shutil\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from random import random\n",
        "from ipywidgets import Image\n",
        "from numpy.linalg import norm\n",
        "from IPython.display import display\n",
        "from keras.models import Model\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# tensorflow version 2.4.0\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input, InceptionV3\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaxCL_01WR-r"
      },
      "source": [
        "# reading from unzipped\n",
        "# è forse leggermente più lento così ma almeno non dobbiamo importare lo zip tutte le volte\n",
        "BASE_DIR = \"/content/gdrive/Shareddrives/COMPUTER_VISION/MIRCV\"\n",
        "# FILELIST_PATH = BASE_DIR + \"/filelist.txt\"\n",
        "SKETCHES_DIR = os.path.join(BASE_DIR, \"sketches\")\n",
        "MIRFLICKR_DIR = os.path.join(BASE_DIR, \"mirflickr/mirflickr25k\")\n",
        "IMG_HEIGHT = 299\n",
        "IMG_WIDTH = 299\n",
        "INPUT_SHAPE = (IMG_WIDTH, IMG_HEIGHT, 3)\n",
        "BATCH_SIZE = 64\n",
        "MODEL_PATH = \"/content/gdrive/Shareddrives/COMPUTER_VISION/models\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEBRySJbXA9F"
      },
      "source": [
        "## Lsh definition\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTiQzbPlWr9h"
      },
      "source": [
        "# def singleton(class_):\n",
        "#     instances = {}\n",
        "#     def getinstance(*args, **kwargs):\n",
        "#         if class_ not in instances:\n",
        "#             instances[class_] = class_(*args, **kwargs)\n",
        "#         return instances[class_]\n",
        "#     return getinstance\n",
        "\n",
        "# @singleton\n",
        "# per adesso è più scomodo che utile averlo come singleton\n",
        "\n",
        "class LSH:\n",
        "  def __init__(self, feature_dim, g = 10, h = 20, w = 4, bitwise_hash = False):\n",
        "    \"\"\"\n",
        "    We have to find a way to load the stored index if exists or initialize the \n",
        "    initial structure\n",
        "    \"\"\"\n",
        "    self._index = {}\n",
        "    if bitwise_hash:\n",
        "      # self.x = np.random.randn(g, h, feature_dim) # non sembra richieda una distribuzione normale\n",
        "      self.x = np.random.normal(size=(g, h, feature_dim))\n",
        "    else:\n",
        "      self.x = np.random.normal(size=(g, h, feature_dim))\n",
        "    self.w = np.ones((g, h, 1)) * w\n",
        "    self.b = np.random.rand(g, h, 1) * w\n",
        "    self.bitwise_hash = bitwise_hash # another way to create h (h will be only 0 o 1)\n",
        "\n",
        "  def _hash(self, features):\n",
        "    \"\"\"\n",
        "    crea l'hash di più cose contemporaeamente si aspetta un array composto dalle features una sotta l'altra (linguaggio super matematico)\n",
        "    \"\"\"\n",
        "    # g = np.trunc((np.dot(p, self.x) + self.b) / self.w)\n",
        "    #g = np.trunc((np.dot(self.x, p) + self.b) / self.w) questo funziona con 1\n",
        "    # g = np.transpose(np.trunc(((np.dot(self.x, p.T) + self.b) / self.w)), (0,2,1)) miglior modo di vederlo\n",
        "    if self.bitwise_hash:\n",
        "      return (np.transpose(np.dot(self.x, features.T), (0,2,1)) > 0).astype(int).astype(str)\n",
        "    return np.transpose(np.trunc(((np.dot(self.x, features.T) + self.b) / self.w)), (0,2,1)).astype(int).astype(str)\n",
        "\n",
        "  def insert(self, features, ids, labels):\n",
        "    \"\"\"\n",
        "    Insert new data, ci aspettiamo un array d\n",
        "    \"\"\"\n",
        "    g = self._hash(features)\n",
        "    assert features.shape[0] == len(ids), \"mismatch between ids length and features\"\n",
        "    assert len(labels) == len(ids), \"mismatch between ids length and labels\"\n",
        "    \n",
        "    number_elements = len(ids)\n",
        "    i = 0\n",
        "    # print(\"hash calculated\")\n",
        "    # print(g.shape)\n",
        "    g_index = -1\n",
        "    for g_function in g:\n",
        "      start_inner_for = time.time()\n",
        "      g_index += 1\n",
        "      for row in g_function:\n",
        "        if i % 10000 == 0:\n",
        "          start = time.time()\n",
        "        \n",
        "        bucket_id = str(g_index) + '_' + ','.join(row)\n",
        "        \n",
        "        if i % 10000 == 0:\n",
        "            end = time.time()\n",
        "            # print(f'join {end - start}')\n",
        "        if not bucket_id in self._index:\n",
        "          # self._index[bucket_id] = { 'features': np.array([features[i % number_elements]]), 'ids': np.array([ids[i % number_elements]]), 'labels': np.array([labels[i % number_elements]])}\n",
        "          self._index[bucket_id] = { 'features': [features[i % number_elements]], 'ids': [ids[i % number_elements]], 'labels': [labels[i % number_elements]]}\n",
        "          if i % 10000 == 0:\n",
        "            end = time.time()\n",
        "            # print(f'not in bucket {end - start}')\n",
        "        else:\n",
        "          if ids[i % number_elements] in self._index[bucket_id]['ids']:\n",
        "            # print(\"duplicate\")\n",
        "            continue\n",
        "          if i % 10000 == 0:\n",
        "            end = time.time()\n",
        "            # print(f'checking duplicates {end - start}')\n",
        "          # print(\"collision inserted\")\n",
        "          # self._index[bucket_id]['features'] = np.vstack((self._index[bucket_id]['features'], features[i % number_elements]))\n",
        "          self._index[bucket_id]['features'].append(features[i % number_elements])\n",
        "          # self._index[bucket_id]['ids'] = np.vstack((self._index[bucket_id]['ids'], ids[i % number_elements]))\n",
        "          self._index[bucket_id]['ids'].append(ids[i % number_elements])\n",
        "          # self._index[bucket_id]['labels'] = np.vstack((self._index[bucket_id]['labels'], labels[i % number_elements]))\n",
        "          self._index[bucket_id]['labels'].append(labels[i % number_elements])\n",
        "          if i % 10000 == 0:\n",
        "            end = time.time()\n",
        "            # print(f'stacking {end - start}')\n",
        "        i += 1\n",
        "        assert i > 0, 'out of bound'\n",
        "      end_inner_for = time.time()\n",
        "      # print(f'inner for time: {end_inner_for - start_inner_for}')\n",
        "    \n",
        "    for bucket_id in self._index:\n",
        "      self._index[bucket_id]['features'] = np.array(self._index[bucket_id]['features'])\n",
        "      # print(self._index[bucket_id]['features'].shape)\n",
        "      self._index[bucket_id]['ids'] = np.array(self._index[bucket_id]['ids'], )\n",
        "      self._index[bucket_id]['labels'] = np.array(self._index[bucket_id]['labels'])\n",
        "      \n",
        "\n",
        "  def query(self, features, top_k, mode = 'euclidean', return_cost = False):\n",
        "    \"\"\"\n",
        "    Query the data\n",
        "    \"\"\"\n",
        "    g = self._hash(np.array([features]))\n",
        "    i = 0\n",
        "    k = None\n",
        "    top_k += 1 # per far ritornare k e non k - 1\n",
        "    cost = 0\n",
        "    g_index = -1\n",
        "    assert mode in ['similarity', 'euclidean'], \"mode must be similarity or euclidean\"\n",
        "    for g_function in g:\n",
        "      g_index += 1\n",
        "      for row in g_function:\n",
        "        bucket_id = str(g_index) + '_' + ','.join(row)\n",
        "        # print(bucket_id)\n",
        "        if bucket_id in self._index:\n",
        "          # posso avere duplicati perchè se i punti vengono inseriti in più bucket, posso avere duplicati\n",
        "          # quindi devo eliminarli\n",
        "          # l'ho messo qua fuori che il controllo duplicati è uguale per tutte e due le distanze\n",
        "          # print(f'bucket {bucket_id}')\n",
        "          if k is not None:\n",
        "            # print(\"duplicate\")\n",
        "            duplicate_index = np.isin(self._index[bucket_id]['ids'], k['ids'])\n",
        "            if duplicate_index.all():\n",
        "              continue; # se sono tutti duplicati non ha senso contare nulla\n",
        "            bucket = {}\n",
        "            bucket['ids'] = self._index[bucket_id]['ids'][~duplicate_index] # prendo quelli che non sono duplicati\n",
        "            # print(duplicate_index)\n",
        "            # print(self._index[bucket_id]['features'])\n",
        "            bucket['features'] = self._index[bucket_id]['features'][~duplicate_index.flatten()] # each duplicate index must delete a row of features\n",
        "            bucket['labels'] = self._index[bucket_id]['labels'][~duplicate_index]\n",
        "          else:\n",
        "            bucket = self._index[bucket_id]\n",
        "        \n",
        "          if mode == 'euclidean':\n",
        "            # print(bucket['features'].shape)\n",
        "            dist = norm(bucket['features'] - np.array(features), axis=1)\n",
        "            # print(f'dist shape {dist.shape} and dist size {dist.size}')\n",
        "            cost += dist.size\n",
        "            if k is None:\n",
        "              idx_partitioned = np.argpartition(dist, top_k - 1 if dist.shape[0] - 1 > top_k - 1 else dist.shape[0] - 1)\n",
        "              if dist.shape[0] - 1 > top_k - 1:  \n",
        "                  idx_partitioned = idx_partitioned[:top_k - 1]\n",
        "              k = {}\n",
        "              # qua è più comodo avere array 1- dimensionali\n",
        "              k['ids'] = bucket['ids'][idx_partitioned].flatten()\n",
        "              k['labels'] = bucket['labels'][idx_partitioned].flatten()\n",
        "              k['distances'] = dist[idx_partitioned]\n",
        "              continue\n",
        "            # https://stackoverflow.com/questions/10337533/a-fast-way-to-find-the-largest-n-elements-in-an-numpy-array\n",
        "            # argpartition sembra essere incredibilmente veloce\n",
        "            # ma non ordina completamente, ordina solo rispetto un punto, nel senso\n",
        "            # io gli sto dicendo butta quelli più piccoli di k da una parte e quelli più grandi all'altra, ma non sto ordinando\n",
        "            if k['distances'].shape[0] < top_k:\n",
        "                # print((k['distances'].shape, dist.shape))\n",
        "                distances = np.concatenate((k['distances'], dist))\n",
        "                # print((k['ids'].shape, bucket['ids'].shape))\n",
        "                # print(k['ids'])\n",
        "                # print(bucket['ids'])\n",
        "                ids = np.concatenate((k['ids'], bucket['ids']))\n",
        "                # print((k['labels'].shape, bucket['labels'].shape))\n",
        "                labels = np.concatenate((k['labels'], bucket['labels']))\n",
        "                idx_sorted = np.argpartition(distances, top_k - 1 if distances.shape[0] - 1 > top_k else distances.shape[0] - 1)\n",
        "                if distances.shape[0] - 1 > top_k - 1:  \n",
        "                  idx_sorted = idx_sorted[:top_k - 1]\n",
        "                k['ids'] = ids[idx_sorted]\n",
        "                k['labels'] = labels[idx_sorted]\n",
        "                k['distances'] = distances[idx_sorted]\n",
        "                # print(f'k = {k}')\n",
        "                continue\n",
        "\n",
        "            idx = dist < np.max(k['distances'])\n",
        "            # print(f\"idx = {idx}\")\n",
        "            if np.any(idx):\n",
        "              distances = np.concatenate((k['distances'], dist[idx]))\n",
        "              ids = np.concatenate((k['ids'], bucket['ids'][idx]))\n",
        "              labels = np.concatenate((k['labels'], bucket['labels'][idx]))\n",
        "              idx_sorted = np.argpartition(distances, top_k - 1 if distances.shape[0] - 1 > top_k else distances.shape[0] - 1)\n",
        "              if distances.shape[0] - 1 > top_k - 1:  \n",
        "                  idx_sorted = idx_sorted[:top_k - 1]\n",
        "              k['ids'] = ids[idx_sorted]\n",
        "              k['labels'] = labels[idx_sorted]\n",
        "              k['distances'] = distances[idx_sorted]\n",
        "\n",
        "          else:\n",
        "            # print(bucket['features'].shape)\n",
        "            sim = np.sum(bucket['features'] * np.array(features), axis=1) / (norm(bucket['features'], axis=1) * norm(np.array([features]), axis=1))\n",
        "            # print(f'sim shape {sim.shape} and sim size {sim.size}')\n",
        "            cost += sim.size\n",
        "            if k is None:\n",
        "              idx_partitioned = np.argpartition(sim, -(top_k - 1) if sim.shape[0] - 1 > top_k - 1 else sim.shape[0] - 1)\n",
        "              if sim.shape[0] - 1 > top_k - 1:  \n",
        "                idx_partitioned = idx_partitioned[-(top_k - 1):]\n",
        "              k = {}\n",
        "              # qua è più comodo avere array 1- dimensionali\n",
        "              k['ids'] = bucket['ids'][idx_partitioned].flatten()\n",
        "              k['labels'] = bucket['labels'][idx_partitioned].flatten()\n",
        "              k['similarities'] = sim[idx_partitioned]\n",
        "              continue\n",
        "            # https://stackoverflow.com/questions/10337533/a-fast-way-to-find-the-largest-n-elements-in-an-numpy-array\n",
        "            # argpartition sembra essere incredibilmente veloce\n",
        "            # ma non ordina completamente, ordina solo rispetto un punto, nel senso\n",
        "            # io gli sto dicendo butta quelli più piccoli di k da una parte e quelli più grandi all'altra, ma non sto ordinando\n",
        "            if k['similarities'].shape[0] < top_k:\n",
        "                # print((k['similarities'].shape, sim.shape))\n",
        "                similarities = np.concatenate((k['similarities'], sim))\n",
        "                # print((k['ids'].shape, bucket['ids'].shape))\n",
        "                # print(k['ids'])\n",
        "                # print(bucket['ids'])\n",
        "                ids = np.concatenate((k['ids'], bucket['ids']))\n",
        "                # print((k['labels'].shape, bucket['labels'].shape))\n",
        "                labels = np.concatenate((k['labels'], bucket['labels']))\n",
        "                idx_sorted = np.argpartition(similarities, -(top_k - 1) if similarities.shape[0] - 1 > top_k - 1 else similarities.shape[0] - 1)\n",
        "                if similarities.shape[0] - 1 > top_k - 1:  \n",
        "                  idx_sorted = idx_sorted[-(top_k - 1):]\n",
        "                k['ids'] = ids[idx_sorted]\n",
        "                k['labels'] = labels[idx_sorted]\n",
        "                k['similarities'] = similarities[idx_sorted]\n",
        "                # print(f'k = {k}')\n",
        "                continue\n",
        "\n",
        "            idx = sim > np.min(k['similarities'])\n",
        "            # print(f\"idx = {idx}\")\n",
        "            if np.any(idx):\n",
        "              similarities = np.concatenate((k['similarities'], sim[idx]))\n",
        "              ids = np.concatenate((k['ids'], bucket['ids'][idx]))\n",
        "              labels = np.concatenate((k['labels'], bucket['labels'][idx]))\n",
        "              idx_sorted = np.argpartition(similarities, -(top_k - 1) if similarities.shape[0] - 1 > top_k - 1 else similarities.shape[0] - 1)\n",
        "              if similarities.shape[0] - 1 > top_k - 1:  \n",
        "                idx_sorted = idx_sorted[-(top_k - 1):]\n",
        "              k['ids'] = ids[idx_sorted]\n",
        "              k['labels'] = labels[idx_sorted]\n",
        "              k['similarities'] = similarities[idx_sorted]\n",
        "        i += 1\n",
        "    # ora ordino totalmente i risultati\n",
        "    if k is None:\n",
        "      return {} #zero result\n",
        "    if mode == 'euclidean':\n",
        "      idx_sorted = np.argsort(k['distances'])\n",
        "      idx_sorted = idx_sorted[:top_k - 1 if k['distances'].shape[0] - 1 > top_k else k['distances'].shape[0]]\n",
        "      k['distances'] = k['distances'][idx_sorted]\n",
        "      k['ids'] = k['ids'][idx_sorted]\n",
        "      k['labels'] = k['labels'][idx_sorted]\n",
        "      if return_cost:\n",
        "        return (k, cost)\n",
        "      return k\n",
        "    idx_sorted = np.argsort(k['similarities'])[::-1]\n",
        "    idx_sorted = idx_sorted[:top_k - 1 if k['similarities'].shape[0] - 1 > top_k else k['similarities'].shape[0]]\n",
        "    k['similarities'] = k['similarities'][idx_sorted]\n",
        "    k['ids'] = k['ids'][idx_sorted]\n",
        "    k['labels'] = k['labels'][idx_sorted]\n",
        "    if return_cost:\n",
        "      return (k, cost)\n",
        "    return k\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Dh5KGCEXH32"
      },
      "source": [
        "## Load models and index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8lo5QunV51B"
      },
      "source": [
        "model = models.load_model(MODEL_PATH + '/inception_finetuning_classification_3_last_one_more_train_all_parameters_more_train.h5') # deve essere la solita con cui sono state estratte le features\n",
        "extractor = Model(model.input, model.layers[-2].output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWHAex9ATSa_"
      },
      "source": [
        "with open('/content/gdrive/Shareddrives/COMPUTER_VISION/saved_index/index.pickle', 'rb') as handle:\n",
        "    lsh = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKlr3s6JXKju"
      },
      "source": [
        "# Start server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgynA6dZXtYL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PJETAwYSPEc",
        "outputId": "16002399-0dff-4262-8268-9c69a6a7bed3"
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, request\n",
        "import base64\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)   #starts ngrok when the app is run\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "  return \"<h1>Running Flask on Google Colab!</h1>\"\n",
        "\n",
        "@app.route('/images/<path:label>')\n",
        "def images(label):\n",
        "  print(label)\n",
        "  if label.startswith('png/'): # mirflick \n",
        "    labels.append(os.path.join(MIRFLICKR_DIR, label))\n",
        "  else: #sketches\n",
        "    labels.append(os.path.join(SKETCHES_DIR, label))\n",
        "  return send_from_directory('js', path)\n",
        "\n",
        "@app.route(\"/query\", methods=['POST'])\n",
        "def query():\n",
        "    image_encoded = request.form['data'].split(',')[1]\n",
        "    image = base64.decodebytes(image_encoded.encode())\n",
        "    im = Image.open(io.BytesIO(image))\n",
        "    network_input = preprocess_input(np.array(im)[:, :, :3]) # remove alpha\n",
        "    print(network_input[np.newaxis, :, :, :].shape)\n",
        "    features = extractor.predict(network_input[np.newaxis, :, :, :])\n",
        "    res = lsh.query(features[0], top_k = 80, mode='similarity')\n",
        "    res['ids'] = res['ids'].tolist()\n",
        "    res['labels'] = res['labels'].tolist()\n",
        "    labels = []\n",
        "    if label.startswith('png/'): # mirflick \n",
        "      labels.append(label)\n",
        "    else: #sketches\n",
        "      labels.append(s.patho.join(SKETCHES_DIR, label))\n",
        "\n",
        "    res['similarities'] = res['similarities'].tolist()\n",
        "\n",
        "    return res\n",
        "\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://f9b42740b6a9.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [24/Jan/2021 17:10:01] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [24/Jan/2021 17:10:01] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [24/Jan/2021 17:10:05] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "[2021-01-24 17:10:14,865] ERROR in app: Exception on /images/png/im1.jpg [GET]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 2447, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 1952, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 1821, in handle_user_exception\n",
            "    reraise(exc_type, exc_value, tb)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/flask/_compat.py\", line 39, in reraise\n",
            "    raise value\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 1950, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 1936, in dispatch_request\n",
            "    return self.view_functions[rule.endpoint](**req.view_args)\n",
            "  File \"<ipython-input-38-243342c7895f>\", line 18, in images\n",
            "    labels.append(os.path.join(MIRFLICKR_DIR, label))\n",
            "NameError: name 'labels' is not defined\n",
            "127.0.0.1 - - [24/Jan/2021 17:10:14] \"\u001b[35m\u001b[1mGET /images/png/im1.jpg HTTP/1.1\u001b[0m\" 500 -\n",
            "127.0.0.1 - - [24/Jan/2021 17:10:15] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "png/im1.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3XfsVTkPR7p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}